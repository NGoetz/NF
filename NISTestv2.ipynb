{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from binNF.normalizing_flows.manager import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    a=torch.zeros_like(x[:,0])\n",
    "    b=torch.ones_like(x[:,0])\n",
    "   \n",
    "    return torch.where(torch.max(abs(x[:,0]), abs(x[:,1]))>1,a,b)\n",
    "\n",
    "def g(x): #box: expect 0.25 in dim2\n",
    "   \n",
    "    q=torch.max(torch.abs(x),dim=-1).values\n",
    "  \n",
    "    a=torch.zeros_like(q)\n",
    "    b=torch.ones_like(q)\n",
    "   \n",
    "    return torch.where(q<0.5,a,b)\n",
    "\n",
    "def gaussian(x):\n",
    "    return torch.exp( -((x[:,0]-0.5)**2+(x[:,1]-0.5)**2)/(0.3**2)) \n",
    "\n",
    "def camel(x):\n",
    "    return torch.exp( -((x[:,0]-0.75)**2+(x[:,1]-0.75)**2)/(0.2**2))+torch.exp( -((x[:,0]-0.25)**2+(x[:,1]-0.25)**2)/(0.2**2))\n",
    "\n",
    "def gaussianb(x):\n",
    "    return torch.exp( -(x)**2)[:,0]\n",
    "\n",
    "def gaussianbnp(x):\n",
    "    return np.exp( -((x[:,0]+1)**2+(x[:,1])**2) )\n",
    "\n",
    "def gaussiannp(x):\n",
    "    return np.exp( -(x[:,0])**2 )\n",
    "\n",
    "def con(x):\n",
    "    y=torch.empty(x.shape[0])\n",
    "    return y.fill_(5)\n",
    "\n",
    "def sin(x):\n",
    "    return 2+torch.sin(x[:,1])\n",
    "\n",
    "def lin(x):\n",
    "    return 0.2*x[:,0]+0.5\n",
    "\n",
    "def sinnp(x):\n",
    "    return 2+np.sin(x[:,1])\n",
    "\n",
    "def tanp(r):\n",
    "    return (1+((torch.tan((r-0.5)*np.pi))**2))*np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_flow = 2      # number of dimensions\n",
    "\n",
    " \n",
    "# We define our NormalizingFlow object \n",
    "NF =  PWQuadManager(n_flow=n_flow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8259, 0.2689],\n",
      "        [0.2952, 0.2497],\n",
      "        [0.1628, 0.3456],\n",
      "        [0.3115, 0.5315],\n",
      "        [0.0461, 0.9352],\n",
      "        [0.3309, 0.9506],\n",
      "        [0.8569, 0.9783],\n",
      "        [0.9107, 0.7157],\n",
      "        [0.4549, 0.6370],\n",
      "        [0.8753, 0.4072]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The model is created\n",
    "NF.create_model(n_pass_through=1,n_cells=2, n_bins=10, NN=[10,10,10,10,10], roll_step=1) \n",
    "optim = torch.optim.Adamax(NF._model.parameters(),lr=1e-4, weight_decay=2.78e-07) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w = torch.empty(10, NF.n_flow)\n",
    "torch.nn.init.uniform_(w)\n",
    "XJ = NF.model(NF.format_input(w,dev=torch.device(\"cpu\")))\n",
    "X = (XJ[:, :-1])\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch save not possible\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loss: 0.000e+00 | Epoch', max=1000.0, style=ProgressStyleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch save not possible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=NF._train_variance_forward_seq(camel,optim,\"./logs/tmp/\",10000,1000,0,True, True,True)\n",
    "\n",
    "\n",
    "#go through coupling cell again, hypop search 1000, choose optimium, investigate variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss\n",
      "tensor(0.1238)\n",
      "Epoch of best result\n",
      "817\n",
      "Best loss\n",
      "tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "Best loss relative\n",
      "tensor(0.6217, grad_fn=<DivBackward0>)\n",
      "Function evaluations\n",
      "8190000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'      \\n#print(NF.model)\\nlosses=[]\\nfor key, value in NF.history.items():\\n    losses.append(value[\"loss\"])\\n\\nfig = plt.figure(figsize=(12, 4))\\na1=fig.add_subplot(131)\\nplt.plot(losses)\\n\\na1.title.set_text(\\'Loss\\')\\na2=fig.add_subplot(132)\\nplt.plot(np.sqrt(np.exp(losses)))\\na2.title.set_text(\\'Standard Deviation\\')\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Initial loss')\n",
    "print(NF.int_loss)\n",
    "print('Epoch of best result')\n",
    "print(NF.best_epoch)\n",
    "print('Best loss')\n",
    "print(NF.best_loss)\n",
    "print('Best loss relative')\n",
    "print(NF.best_loss_rel)\n",
    "print('Function evaluations')\n",
    "print(NF.best_func_count)\n",
    "\"\"\"      \n",
    "#print(NF.model)\n",
    "losses=[]\n",
    "for key, value in NF.history.items():\n",
    "    losses.append(value[\"loss\"])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "a1=fig.add_subplot(131)\n",
    "plt.plot(losses)\n",
    "\n",
    "a1.title.set_text('Loss')\n",
    "a2=fig.add_subplot(132)\n",
    "plt.plot(np.sqrt(np.exp(losses)))\n",
    "a2.title.set_text('Standard Deviation')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "tensor([[0.8735, 0.8214],\n",
      "        [0.3355, 0.7534],\n",
      "        [0.3476, 0.9583],\n",
      "        [0.6777, 0.0925],\n",
      "        [0.5267, 0.8377],\n",
      "        [0.0271, 0.6551],\n",
      "        [0.2251, 0.5799],\n",
      "        [0.8015, 0.2300],\n",
      "        [0.6072, 0.7349],\n",
      "        [0.7512, 0.8764],\n",
      "        [0.7042, 0.7676],\n",
      "        [0.1980, 0.8055],\n",
      "        [0.2014, 0.2365],\n",
      "        [0.1202, 0.4900],\n",
      "        [0.5486, 0.8155],\n",
      "        [0.2920, 0.0696],\n",
      "        [0.2912, 0.3334],\n",
      "        [0.2302, 0.3249],\n",
      "        [0.1362, 0.2848],\n",
      "        [0.0941, 0.2388],\n",
      "        [0.5034, 0.8582],\n",
      "        [0.4246, 0.2707],\n",
      "        [0.7441, 0.9688],\n",
      "        [0.3914, 0.8287],\n",
      "        [0.7314, 0.9466],\n",
      "        [0.3634, 0.6738],\n",
      "        [0.5428, 0.6822],\n",
      "        [0.9415, 0.6112],\n",
      "        [0.9460, 0.2324],\n",
      "        [0.8106, 0.2091],\n",
      "        [0.9338, 0.6363],\n",
      "        [0.6436, 0.8144],\n",
      "        [0.9323, 0.7901],\n",
      "        [0.3414, 0.5618],\n",
      "        [0.8846, 0.1560],\n",
      "        [0.5221, 0.2339],\n",
      "        [0.4105, 0.2649],\n",
      "        [0.7066, 0.6571],\n",
      "        [0.7210, 0.1155],\n",
      "        [0.1277, 0.2099]])\n",
      "with jacob\n",
      "tensor([[0.8735, 0.8214, 1.0000],\n",
      "        [0.3355, 0.7534, 1.0000],\n",
      "        [0.3476, 0.9583, 1.0000],\n",
      "        [0.6777, 0.0925, 1.0000],\n",
      "        [0.5267, 0.8377, 1.0000],\n",
      "        [0.0271, 0.6551, 1.0000],\n",
      "        [0.2251, 0.5799, 1.0000],\n",
      "        [0.8015, 0.2300, 1.0000],\n",
      "        [0.6072, 0.7349, 1.0000],\n",
      "        [0.7512, 0.8764, 1.0000],\n",
      "        [0.7042, 0.7676, 1.0000],\n",
      "        [0.1980, 0.8055, 1.0000],\n",
      "        [0.2014, 0.2365, 1.0000],\n",
      "        [0.1202, 0.4900, 1.0000],\n",
      "        [0.5486, 0.8155, 1.0000],\n",
      "        [0.2920, 0.0696, 1.0000],\n",
      "        [0.2912, 0.3334, 1.0000],\n",
      "        [0.2302, 0.3249, 1.0000],\n",
      "        [0.1362, 0.2848, 1.0000],\n",
      "        [0.0941, 0.2388, 1.0000],\n",
      "        [0.5034, 0.8582, 1.0000],\n",
      "        [0.4246, 0.2707, 1.0000],\n",
      "        [0.7441, 0.9688, 1.0000],\n",
      "        [0.3914, 0.8287, 1.0000],\n",
      "        [0.7314, 0.9466, 1.0000],\n",
      "        [0.3634, 0.6738, 1.0000],\n",
      "        [0.5428, 0.6822, 1.0000],\n",
      "        [0.9415, 0.6112, 1.0000],\n",
      "        [0.9460, 0.2324, 1.0000],\n",
      "        [0.8106, 0.2091, 1.0000],\n",
      "        [0.9338, 0.6363, 1.0000],\n",
      "        [0.6436, 0.8144, 1.0000],\n",
      "        [0.9323, 0.7901, 1.0000],\n",
      "        [0.3414, 0.5618, 1.0000],\n",
      "        [0.8846, 0.1560, 1.0000],\n",
      "        [0.5221, 0.2339, 1.0000],\n",
      "        [0.4105, 0.2649, 1.0000],\n",
      "        [0.7066, 0.6571, 1.0000],\n",
      "        [0.7210, 0.1155, 1.0000],\n",
      "        [0.1277, 0.2099, 1.0000]])\n",
      "XJ\n",
      "tensor([[ 0.8306,  1.4394,  0.8861],\n",
      "        [ 0.7065,  0.6224,  0.9194],\n",
      "        [ 0.9715,  0.5010,  0.8781],\n",
      "        [ 0.1472,  0.7407,  0.7388],\n",
      "        [ 0.8620,  0.7716,  0.5798],\n",
      "        [ 0.6354,  0.0923,  1.1683],\n",
      "        [ 0.4667,  0.2470,  0.5377],\n",
      "        [ 0.3193,  0.9909,  2.7053],\n",
      "        [ 0.7364,  0.8303,  0.3279],\n",
      "        [ 0.9491,  1.1778,  0.6369],\n",
      "        [ 0.8583,  1.0004,  0.4050],\n",
      "        [ 0.7784,  0.4284,  3.0028],\n",
      "        [ 0.1528,  0.2089,  0.2752],\n",
      "        [ 0.4305,  0.1453,  0.7453],\n",
      "        [ 0.8978,  0.7848,  0.3659],\n",
      "        [ 0.0602,  0.3112,  0.3588],\n",
      "        [ 0.1863,  0.2543,  0.1681],\n",
      "        [ 0.2058,  0.2192,  0.1738],\n",
      "        [ 0.1971,  0.1452,  0.2784],\n",
      "        [ 0.1954,  0.1197,  0.4707],\n",
      "        [ 0.8983,  0.7500,  1.0568],\n",
      "        [ 0.3615,  0.4835,  1.3554],\n",
      "        [ 1.0195,  1.3042,  0.2916],\n",
      "        [ 0.9315,  0.3926,  0.9359],\n",
      "        [ 1.0062,  1.1515,  0.3060],\n",
      "        [ 0.7524,  0.9038,  0.2169],\n",
      "        [ 0.7517,  1.1253,  0.4617],\n",
      "        [ 0.6619,  2.1832,  1.6897],\n",
      "        [ 0.2979,  1.6301, 12.6870],\n",
      "        [ 0.2610,  0.2482,  0.4683],\n",
      "        [ 0.6807,  1.9089,  2.6893],\n",
      "        [ 0.8258,  0.9611,  0.2918],\n",
      "        [ 0.8164,  1.6250,  0.5142],\n",
      "        [ 0.5160,  0.7274,  0.6040],\n",
      "        [ 0.1641,  1.0354,  1.9691],\n",
      "        [ 0.3107,  0.5856,  1.2646],\n",
      "        [ 0.3587,  0.4667,  1.5130],\n",
      "        [ 0.8009,  1.0741,  0.6396],\n",
      "        [ 0.1600,  0.8126,  1.1830],\n",
      "        [ 0.1606,  0.1473,  0.4879]], grad_fn=<CatBackward>)\n",
      "XJ2\n",
      "tensor([[0.8303, 1.4466, 0.8891],\n",
      "        [0.7053, 0.6224, 0.9290],\n",
      "        [0.9702, 0.5059, 0.9187],\n",
      "        [0.1484, 0.7409, 0.7401],\n",
      "        [0.8631, 0.7740, 0.6159],\n",
      "        [0.6348, 0.0904, 1.1468],\n",
      "        [0.4634, 0.2399, 0.5325],\n",
      "        [0.3247, 0.9475, 2.7659],\n",
      "        [0.7362, 0.8340, 0.3455],\n",
      "        [0.9458, 1.1926, 0.6339],\n",
      "        [0.8703, 0.9916, 0.3550],\n",
      "        [0.7762, 0.4346, 3.0711],\n",
      "        [0.1523, 0.2089, 0.2768],\n",
      "        [0.4308, 0.1455, 0.7454],\n",
      "        [0.8959, 0.7897, 0.3888],\n",
      "        [0.0599, 0.3113, 0.3605],\n",
      "        [0.1859, 0.2542, 0.1696],\n",
      "        [0.2052, 0.2189, 0.1743],\n",
      "        [0.1989, 0.1436, 0.2810],\n",
      "        [0.1962, 0.1191, 0.4698],\n",
      "        [0.8982, 0.7544, 1.1206],\n",
      "        [0.3619, 0.4853, 1.3747],\n",
      "        [1.0159, 1.3133, 0.2970],\n",
      "        [0.9203, 0.5479, 0.8982],\n",
      "        [1.0020, 1.1502, 0.3203],\n",
      "        [0.7483, 0.8180, 0.2345],\n",
      "        [0.7518, 1.1344, 0.4375],\n",
      "        [0.6637, 2.1490, 1.1544],\n",
      "        [0.3009, 2.0355, 7.7405],\n",
      "        [0.2632, 0.2358, 0.4672],\n",
      "        [0.6796, 1.8273, 2.2974],\n",
      "        [0.8273, 0.9667, 0.2714],\n",
      "        [0.8149, 1.6368, 0.5179],\n",
      "        [0.5145, 0.7200, 0.6146],\n",
      "        [0.1638, 1.0305, 1.9548],\n",
      "        [0.3120, 0.5510, 1.2687],\n",
      "        [0.3539, 0.4731, 1.4984],\n",
      "        [0.8107, 1.0512, 0.6478],\n",
      "        [0.1597, 0.8100, 1.2029],\n",
      "        [0.1610, 0.1472, 0.4937]], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nf=gaussian\\nprint(\"f(x)\")\\nprint(f(X))\\n\\n\\nprint(\"fXJ\")\\nprint(torch.mul(f(X), XJ[:, -1]))\\n\\nprint(\"loss without jac\")\\nprint(torch.mean(f(X)**2))\\n\\nprint(\"squared mean\")\\nprint(torch.mean(f(X))**2)\\n\\nprint(\"loss\")\\nprint(torch.mean(torch.mul(f(X), XJ[:, -1])**2))\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w = torch.empty(40, NF.n_flow)\n",
    "torch.nn.init.uniform_(w)\n",
    "\n",
    "print(\"W\")\n",
    "print(w)\n",
    "                    #print(self.format_input(w).requires_grad)\n",
    "print(\"with jacob\")                  # Output a sample of (phase-space point, forward Jacobian)\n",
    "print(NF.format_input(w,torch.device(\"cpu\")))\n",
    "\n",
    "#print(\"mapped\")\n",
    "#print(NF.format_input(100*torch.tan((w-0.5)*(np.pi))))\n",
    "XJ=NF.model(NF.format_input(w,torch.device(\"cpu\")))\n",
    "\n",
    "\n",
    "                    # Separate the points and their Jacobians:\n",
    "print(\"XJ\")\n",
    "print(XJ)# This sample is fixed, we optimize the Jacobian\n",
    "\n",
    "XJ2=NF.best_model(NF.format_input(w,torch.device(\"cpu\")))\n",
    "\n",
    "\n",
    "                    # Separate the points and their Jacobians:\n",
    "print(\"XJ2\")\n",
    "print(XJ2)\n",
    "\n",
    "\"\"\"\n",
    "f=gaussian\n",
    "print(\"f(x)\")\n",
    "print(f(X))\n",
    "\n",
    "\n",
    "print(\"fXJ\")\n",
    "print(torch.mul(f(X), XJ[:, -1]))\n",
    "\n",
    "print(\"loss without jac\")\n",
    "print(torch.mean(f(X)**2))\n",
    "\n",
    "print(\"squared mean\")\n",
    "print(torch.mean(f(X))**2)\n",
    "\n",
    "print(\"loss\")\n",
    "print(torch.mean(torch.mul(f(X), XJ[:, -1])**2))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-10,10,100)\n",
    "ys = np.linspace(10,-10,100) # in images the y axis is inverted\n",
    "Xs,Ys = np.meshgrid(xs,ys)\n",
    "zs=gaussianbnp(np.array(list(zip(Xs.reshape(100*100),Ys.reshape(100*100)))).astype(np.float32)).reshape(100,100)\n",
    "plt.imshow(zs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "W\n",
      "tensor([[0.6108, 0.7451],\n",
      "        [0.3028, 0.2412],\n",
      "        [0.2823, 0.1151],\n",
      "        ...,\n",
      "        [0.5725, 0.1838],\n",
      "        [0.8225, 0.9309],\n",
      "        [0.1988, 0.2423]], requires_grad=True)\n",
      "with jacob\n",
      "tensor([[0.6108, 0.7451, 1.0000],\n",
      "        [0.3028, 0.2412, 1.0000],\n",
      "        [0.2823, 0.1151, 1.0000],\n",
      "        ...,\n",
      "        [0.5725, 0.1838, 1.0000],\n",
      "        [0.8225, 0.9309, 1.0000],\n",
      "        [0.1988, 0.2423, 1.0000]], grad_fn=<CatBackward>)\n",
      "XJ\n",
      "tensor([[0.6108, 0.7238, 0.7407],\n",
      "        [0.3028, 0.2571, 1.1249],\n",
      "        [0.2823, 0.1301, 1.1378],\n",
      "        ...,\n",
      "        [0.5725, 0.1827, 1.4329],\n",
      "        [0.8225, 0.9263, 1.0666],\n",
      "        [0.1988, 0.2883, 0.6346]], grad_fn=<CatBackward>)\n",
      "X\n",
      "tensor([[0.6108, 0.7238],\n",
      "        [0.3028, 0.2571],\n",
      "        [0.2823, 0.1301],\n",
      "        ...,\n",
      "        [0.5725, 0.1827],\n",
      "        [0.8225, 0.9263],\n",
      "        [0.1988, 0.2883]])\n",
      "diff\n",
      "tensor([-0.1342,  0.0616,  0.1672,  ...,  0.3886, -0.1084, -0.0435],\n",
      "       grad_fn=<SubBackward0>)\n",
      "f(x)\n",
      "tensor([2.6622, 2.2543, 2.1297,  ..., 2.1817, 2.7994, 2.2843])\n",
      "fXJ\n",
      "tensor([1.9718, 2.5359, 2.4232,  ..., 3.1262, 2.9860, 1.4496],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"reset\")\n",
    "NF.create_model(n_pass_through=1,n_cells=1,n_bins=10, nn_width=2, NN=[20,20,20], roll_step=0)\n",
    "\n",
    "w = torch.empty(10000, NF.n_flow, requires_grad=True)\n",
    "torch.nn.init.uniform_(w,0,1)\n",
    "\n",
    "print(\"W\")\n",
    "print(w)\n",
    "                    #print(self.format_input(w).requires_grad)\n",
    "print(\"with jacob\")                  # Output a sample of (phase-space point, forward Jacobian)\n",
    "print(NF.format_input(w))\n",
    "XJ = NF.model(NF.format_input(w))\n",
    "\n",
    "                    # Separate the points and their Jacobians:\n",
    "print(\"XJ\")\n",
    "print(XJ)# This sample is fixed, we optimize the Jacobian\n",
    "X = ((XJ[:, :-1]).detach())\n",
    "print(\"X\")\n",
    "print(X)\n",
    "print(\"diff\")\n",
    "print(X[:,0]-w[:,1])\n",
    "print(\"f(x)\")\n",
    "print(sin(X))\n",
    "\n",
    "print(\"fXJ\")\n",
    "print(torch.mul(sin(X), XJ[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#a4=fig.add_subplot(143)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(X[:,0],X[:,1],X[:,2])\n",
    "#plt.2d(X[:,0],X[:,1],X[:,2])\n",
    "#ax.set_aspect(aspect=1.)\n",
    "ax.title.set_text('Jacobian after training')\n",
    "ax.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=True,      # ticks along the bottom edge are off\n",
    "    left=True,         # ticks along the top edge are off\n",
    "    labelbottom=True,\n",
    "    labelleft=True)\n",
    "\n",
    "\n",
    "NF.create_model(n_pass_through=1,n_cells=1, nn_width=2, NN=[5], roll_step=1)\n",
    "rcParams['axes.labelpad'] = 800\n",
    "z = torch.empty((10000,2)) \n",
    "torch.nn.init.uniform_(z,0,1)\n",
    "\n",
    "w=NF.format_input(z)\n",
    "X=NF.model(w).data.numpy()\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(211, projection='3d')\n",
    "ax1.scatter3D(X[:,0],X[:,1],X[:,2])\n",
    "#plt.2d(X[:,0],X[:,1],X[:,2])\n",
    "#ax.set_aspect(aspect=1.)\n",
    "ax1.title.set_text('Jacobian before training')\n",
    "ax1.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=True,      # ticks along the bottom edge are off\n",
    "    left=True,         # ticks along the top edge are off\n",
    "    labelbottom=True,\n",
    "    labelleft=True)\n",
    "#ax1.auto_scale_xyz([0.5, 2], [0, 1], [1.55, 1.75])\n",
    "ax1.xaxis._axinfo['label']['space_factor'] = 4.8\n",
    "ax1.zaxis._axinfo['label']['space_factor'] = 8.8\n",
    "rcParams['axes.labelpad'] = 80\n",
    "\n",
    "print(\"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0788, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = torch.empty((12100,2)) \n",
    "torch.nn.init.uniform_(w,0,1)\n",
    "#q = torch.empty((12100,2)) \n",
    "#torch.nn.init.normal_(q,std=10)\n",
    "#z=150*torch.tan((w-0.5)*(np.pi))\n",
    "#model=torch.load('torch',map_location=torch.device('cpu'))\n",
    "#NF.best_model.load_state_dict(model['model_state_dict'])\n",
    "Y=NF.format_input(w, dev=torch.device(\"cpu\"))\n",
    "X=NF.model(Y)\n",
    "XZ=NF.best_model(Y)\n",
    "#Z=(torch.atan(X)/np.pi+0.5).data.numpy()\n",
    "#print(Z)\n",
    "X=X.data.numpy()\n",
    "XX=XZ.data.numpy()\n",
    "#z=z.data.numpy()\n",
    "#a3=fig.add_subplot(133)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "a3=fig.add_subplot(121)\n",
    "\n",
    "plt.hist2d(XX[:,0],XX[:,1],bins=25)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0,1]) \n",
    "axes.set_ylim([-0,1])\n",
    "#a3.set_aspect(aspect=1.)\n",
    "\n",
    "\n",
    "a3.title.set_text('Point histogram (PDF)')\n",
    "a3.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=True,      # ticks along the bottom edge are off\n",
    "    left=True,         # ticks along the top edge are off\n",
    "    labelbottom=True,\n",
    "    labelleft=True)\n",
    "\n",
    "\n",
    "#fig = plt.figure(figsize=(6, 6))\n",
    "a4=fig.add_subplot(122)\n",
    "#plt.hist2d(X[:,0],X[:,1],bins=2500)\n",
    "#Q=[[]]\n",
    "#Q=[[-1,-1,-1],[2,2,2]]\n",
    "#Q[:,1]=[,]\n",
    "#Q=np.ones((10,2))\n",
    "#Q[:,0]*=-1\n",
    "plt.hist2d(XX[:,0],XX[:,1],bins=10)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0,1])\n",
    "axes.set_ylim([-0,1])\n",
    "#a3.set_aspect(aspect=1.)\n",
    "\n",
    "\n",
    "a4.title.set_text('Point histogram (PDF)')\n",
    "a4.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=True,      # ticks along the bottom edge are off\n",
    "    left=True,         # ticks along the top edge are off\n",
    "    labelbottom=True,\n",
    "    labelleft=True)\n",
    "\n",
    "f=camel\n",
    "\n",
    "w = torch.empty(10800, NF.n_flow)\n",
    "torch.nn.init.uniform_(w)\n",
    "XJ = NF.best_model(NF.format_input(w,dev=torch.device(\"cpu\")))\n",
    "X = (XJ[:, :-1])\n",
    "fXJ = torch.mul(f(X), XJ[:, -1])\n",
    "loss = torch.mean(fXJ**2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(20, 2)\n",
    "torch.nn.init.normal_(w, std=100)\n",
    "w.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bc9878977896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
